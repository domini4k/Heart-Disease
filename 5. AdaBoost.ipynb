{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# AdaBoost"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import preprocessing\nfrom sklearn import datasets ## Get dataset from sklearn\nimport sklearn.model_selection as ms\nimport sklearn.metrics as sklm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport numpy.random as nr\n\n%matplotlib inline",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this notebook AdaBoost method will be used to predict heart disease."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Features = np.array(pd.read_csv('Features.csv'))\nLabels = np.array(pd.read_csv('Labels.csv'))\nLabels = Labels.reshape(Labels.shape[0],)\nprint(Features.shape)\nprint(Labels.shape)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(297, 29)\n(297,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(123)\ninside = ms.KFold(n_splits=10, shuffle = True)\nnr.seed(321)\noutside = ms.KFold(n_splits=10, shuffle = True)",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Boosting is basically using many weak classifiers and weight them properly. If a classifier is causing only a few errors it has a great value (weight). Otherwise the weight is adequactly smaller. The code below searches for the best parameter (like in previous notebooks) for learing rate. Each of elements of the main classifier will be affected by learning rate."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Define the dictionary for the grid search and the model object to search on\nparam_grid = {\"learning_rate\": [0.01, 0.1, 1, 10]}\n## Define the AdaBoosted tree model\nnr.seed(3456)\nab_clf = AdaBoostClassifier()  \n\n## Perform the grid search over the parameters\nnr.seed(4455)\nab_clf = ms.GridSearchCV(estimator = ab_clf, param_grid = param_grid, \n                      cv = inside, # Use the inside folds\n                      scoring = 'roc_auc',\n                      return_train_score = True)\nab_clf.fit(Features, Labels)\nprint(ab_clf.best_estimator_.learning_rate)",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.1\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(498)\ncv_estimate = ms.cross_val_score(ab_clf, Features, Labels, \n                                 cv = outside) # Use the outside folds\n\nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Mean performance metric = 0.905\nSDT of the metric       = 0.057\nOutcomes by cv fold\nFold  1    0.920\nFold  2    0.864\nFold  3    0.814\nFold  4    0.884\nFold  5    0.898\nFold  6    0.818\nFold  7    0.964\nFold  8    0.943\nFold  9    0.980\nFold 10    0.964\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Randomly sample cases to create independent training and test data\nnr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 80)\nX_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(1115)\nab_mod = AdaBoostClassifier(learning_rate = ab_clf.best_estimator_.learning_rate) \nab_mod.fit(X_train, y_train)",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n          learning_rate=0.1, n_estimators=50, random_state=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n\ndef print_metrics(labels, probs, threshold):\n    scores = score_model(probs, threshold)\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n    \nprobabilities = ab_mod.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.55)    ",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                 Confusion matrix\n                 Score positive    Score negative\nActual positive        31                 2\nActual negative        18                29\n\nAccuracy        0.75\nAUC             0.91\nMacro precision 0.78\nMacro recall    0.78\n \n           Positive      Negative\nNum case       33            47\nPrecision    0.63          0.94\nRecall       0.94          0.62\nF1           0.76          0.74\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The results are not great. Note that treshold is already adjusted to value 0.55. Without this change the result would be worse. But still there are two cases with disease that were missclassified.\n\nA huge disadvantage of the AdaBoost method is that the model cannot be weighted properly. As written before the model is unbalanced. Only thing one can do is undersample healthy cases."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "temp_Labels_1 = Labels[Labels == 0]  # Save these\ntemp_Features_1 = Features[Labels == 0,:] # Save these\ntemp_Labels_0 = Labels[Labels == 1]  # Undersample these\ntemp_Features_0 = Features[Labels == 1,:] # Undersample these\n\nindx = nr.choice(temp_Features_0.shape[0], temp_Features_1.shape[0], replace=True)\n\ntemp_Features = np.concatenate((temp_Features_1, temp_Features_0[indx,:]), axis = 0)\ntemp_Labels = np.concatenate((temp_Labels_1, temp_Labels_0[indx,]), axis = 0) \n\nprint(np.bincount(temp_Labels))\nprint(temp_Features.shape)\nprint(temp_Labels.shape)",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[136 136]\n(272, 29)\n(272,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(1234)\ninside = ms.KFold(n_splits=10, shuffle = True)\nnr.seed(3214)\noutside = ms.KFold(n_splits=10, shuffle = True)\n\n## Define the AdaBoosted tree model\nnr.seed(3456)\nab_clf = AdaBoostClassifier()  \n\n## Perform the grid search over the parameters\nnr.seed(4455)\nab_clf = ms.GridSearchCV(estimator = ab_clf, param_grid = param_grid, \n                      cv = inside, # Use the inside folds\n                      scoring = 'roc_auc',\n                      return_train_score = True)\nab_clf.fit(temp_Features, temp_Labels)\nprint(ab_clf.best_estimator_.learning_rate)",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.1\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(498)\ncv_estimate = ms.cross_val_score(ab_clf, Features, Labels, \n                                 cv = outside) # Use the outside folds\n\nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Mean performance metric = 0.905\nSDT of the metric       = 0.057\nOutcomes by cv fold\nFold  1    0.920\nFold  2    0.864\nFold  3    0.814\nFold  4    0.884\nFold  5    0.898\nFold  6    0.818\nFold  7    0.964\nFold  8    0.943\nFold  9    0.980\nFold 10    0.964\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Randomly sample cases to create independent training and test data\nnr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 80)\nX_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])\n\n## Undersample the majority case for the training data\ntemp_Labels_1 = y_train[y_train == 0]  # Save these\ntemp_Features_1 = X_train[y_train == 0,:] # Save these\ntemp_Labels_0 = y_train[y_train == 1]  # Undersample these\ntemp_Features_0 = X_train[y_train == 1,:] # Undersample these\n\nindx = nr.choice(temp_Features_0.shape[0], temp_Features_1.shape[0], replace=True)\n\nX_train = np.concatenate((temp_Features_1, temp_Features_0[indx,:]), axis = 0)\ny_train = np.concatenate((temp_Labels_1, temp_Labels_0[indx,]), axis = 0) \n\nprint(np.bincount(y_train))\nprint(X_train.shape)\nprint(y_train.shape)",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[103 103]\n(206, 29)\n(206,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Define and fit the model\nnr.seed(1115)\nab_mod = AdaBoostClassifier(learning_rate = ab_clf.best_estimator_.learning_rate) \nab_mod.fit(X_train, y_train)",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n          learning_rate=0.1, n_estimators=50, random_state=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "probabilities = ab_mod.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.55)    ",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                 Confusion matrix\n                 Score positive    Score negative\nActual positive        31                 2\nActual negative        18                29\n\nAccuracy        0.75\nAUC             0.91\nMacro precision 0.78\nMacro recall    0.78\n \n           Positive      Negative\nNum case       33            47\nPrecision    0.63          0.94\nRecall       0.94          0.62\nF1           0.76          0.74\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With the evaluation in the cells above one cannot see a great improvement. Furthermore, in this particular dataset and with this particular goal (to classify all the cases with disease properly and as less cases without disease as disease as possible) this is not the best method. It has to be a method where it is possible to specify how important are cases with disease, way more important than cases without disease. As written before those cases would be only an additional job for a doctor to specify if the case is ill or not. But the important thing is not to miss a single case with disease. \n\nOf all three methods shown on 3rd 4th and 5th notebook the best performance was regression (3rd notebook).  "
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}